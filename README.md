# ğŸ“Š TFM CienciometrÃ­a
**Ãndice de novedad en publicaciones cientÃ­ficas del campo de las matemÃ¡ticas**

Este proyecto forma parte del Trabajo de Fin de MÃ¡ster y tiene como objetivo **estimar un Ã­ndice de novedad en artÃ­culos cientÃ­ficos del Ã¡rea de matemÃ¡ticas**, a partir del anÃ¡lisis semÃ¡ntico de tÃ­tulos y resÃºmenes.  
Se utilizan enfoques de **Word2Vec** y **MathBERT**, junto con tÃ©cnicas de cienciometrÃ­a.

---

## ğŸ“‚ Estructura del repositorio

```
TFM_Cienciometria/
â”œâ”€â”€ logs/                  # Contiene los logs del proyecto
â”‚            
â”œâ”€â”€ scripts/               # Scripts de procesamiento
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â”œâ”€â”€ word2Vec/      # Scripts para pipeline Word2Vec
â”‚   â”‚   â”œâ”€â”€ mathbert/      # Notebook especÃ­fico de MathBERT
â”‚   â”‚   â”‚   â””â”€â”€ 04_mathbert_vectorizar.ipynb   # Se ejecuta en Colab
â”‚   â”‚   â”œâ”€â”€ 05_computar_novedad_word2vec_mathbert.py  # Se ejecuta en Colab
â”‚   â”‚   â””â”€â”€ 06_visualizar_novedad_word2vec_mathbert.ipynb # Se ejecuta en Colab
â”‚   â””â”€â”€ recoleccionDatosAPI/ # Descarga desde API de Scopus
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
```

---

## âš™ï¸ InstalaciÃ³n

1. Clona el repositorio:
   ```bash
   git clone https://github.com/usuario/TFM_Cienciometria.git
   cd TFM_Cienciometria
   ```

2. Crea un entorno virtual e instala dependencias:
   ```bash
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   venv\Scripts\activate     # Windows

   pip install -r requirements.txt
   ```

3. **Modelos externos necesarios**:
   - Modelo de idioma FastText (`lid.176.ftz`) se descarga automÃ¡ticamente en `/idioma_modelo/` cuando se ejecuta `01_filter_language.py`.
   - Modelo de spaCy:
     ```bash
     python -m spacy download en_core_web_sm
     ```

---

## ğŸš€ Pipeline de procesamiento

### 1. RecolecciÃ³n de datos desde Scopus
Los scripts en `/scripts/recoleccionDatosAPI/` descargan y enriquecen los artÃ­culos:
```bash
python scripts/recoleccionDatosAPI/searchAPI/main_06.py
python scripts/recoleccionDatosAPI/abstractAPI/enriquecer_desde_eid.py
```

### 2. Preprocesamiento de abstracts (Word2Vec)
```bash
cd scripts/embeddings/word2Vec

# Filtrado por idioma (usa FastText)
python 01_filter_language.py --todo

# Preprocesamiento y bigramas
python 02_preprocess_text.py --todo

# Entrenamiento Word2Vec
python 03_train_word2vec.py --todo

# VectorizaciÃ³n de documentos
python 04_vectorizar_word2vec.py --todo
```

### 3. VectorizaciÃ³n con MathBERT
En `/scripts/embeddings/mathbert/`, se ejecuta en Google Colab:
- **`04_mathbert_vectorizar.ipynb`**

### 4. CÃ¡lculo de novedad (Word2Vec + MathBERT)
Los dos notebooks compartidos para ambos enfoques (ejecutar en Colab):
- **`05_computar_disrupcion_word2vec_mathbert.ipynb`**
- **`06_visualizar_disrupciÃ³n_word2vec_mathbert.ipynb`**

---

## ğŸ’¾ Datos en Zenodo
Para cumplir con los tÃ©rminos de uso de la API de Scopus, este repositorio no contiene los metadatos brutos de los artÃ­culos. En su lugar, se publica en **Zenodo** el conjunto de datos oficial del proyecto, que consiste en la lista completa de identificadores (EIDs) de Scopus utilizados.

Publicar los EIDs garantiza la **total reproducibilidad** de la investigaciÃ³n, permitiendo que cualquier persona con acceso a Scopus pueda reconstruir el corpus original.

El dataset cuenta con un **DOI (Digital Object Identifier)**, lo que lo convierte en un recurso citable y permanente.

â¡ï¸ **Accede y cita el conjunto de datos aquÃ­:**

[![DOI]](https://doi.org/10.5281/zenodo.17445712)

---

## ğŸ“‘ Requisitos principales
- `fasttext==0.9.2`
- `gensim==4.3.3`
- `spacy==3.8.7`
- `torch==2.5.0`
- `transformers==4.44.2`
- `pandas`, `numpy`, `tqdm`, `matplotlib`, `seaborn`

(Ver [requirements.txt](requirements.txt) para la lista completa).

---

## âœ¨ CrÃ©ditos
Trabajo de Fin de MÃ¡ster en CienciometrÃ­a.  
Autor: Jorge GÃ³mez GÃ³mez