{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b11f75a1d09b46d6bb429ec199e1f8f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "Seleccione un modelo",
              "mathbert",
              "word2vec"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Modelo:",
            "description_tooltip": null,
            "disabled": false,
            "index": 2,
            "layout": "IPY_MODEL_dc46d5e446e245bc9e1d6b630fbffb1b",
            "style": "IPY_MODEL_7578bd208b414a93ad84f85727824614"
          }
        },
        "dc46d5e446e245bc9e1d6b630fbffb1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7578bd208b414a93ad84f85727824614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Celda de Configuraci√≥n Inicial para Google Colab\n",
        "# ==========================================================\n",
        "# Ejecuta esta celda al principio de tu notebook cada vez que inicies una nueva sesi√≥n.\n",
        "\n",
        "# --- 1. Conectar a Google Drive ---\n",
        "# Aparecer√° una ventana para que autorices el acceso a tus archivos.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"\\n‚úÖ Google Drive conectado exitosamente.\")\n",
        "\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlmnM_LhNy76",
        "outputId": "d340ce5b-089e-4468-8a85-2acac1c91761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "‚úÖ Google Drive conectado exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# SECCI√ìN DE CONFIGURACI√ìN INTERACTIVA\n",
        "# ==========================================================\n",
        "\n",
        "# Opci√≥n 1: Men√∫ Desplegable (RECOMENDADO)\n",
        "model_options = ['Seleccione un modelo', 'mathbert', 'word2vec']\n",
        "model_selector = widgets.Dropdown(\n",
        "    options=model_options,\n",
        "    value=model_options[0],\n",
        "    description='Modelo:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "print(\"Por favor, selecciona un modelo para el cual calcular el √çndice KI y luego ejecuta la celda de abajo.\")\n",
        "display(model_selector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "b11f75a1d09b46d6bb429ec199e1f8f1",
            "dc46d5e446e245bc9e1d6b630fbffb1b",
            "7578bd208b414a93ad84f85727824614"
          ]
        },
        "id": "sSF-G-JTZqlu",
        "outputId": "29ebdf07-f903-4b15-d92e-c57906502bc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Por favor, selecciona un modelo para el cual calcular el √çndice KI y luego ejecuta la celda de abajo.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Modelo:', options=('Seleccione un modelo', 'mathbert', 'word2vec'), value='Seleccione un‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b11f75a1d09b46d6bb429ec199e1f8f1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6Rs1FhsEaOj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6562a5d-2100-4263-d5a6-703c16553d3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Modelo seleccionado: word2vec\n",
            "‚úÖ Ventana de c√°lculo para KI: 5 a√±os (pasado y futuro)\n",
            "==================================================\n",
            "INICIANDO C√ÅLCULO PARA EL MODELO: word2vec\n",
            "CARGANDO VECTORES DESDE: /content/drive/MyDrive/TFM_Cienciometria/data/word2vec/tmp/04_vectores_word2vec_TODO.pkl\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ==========================================================\n",
        "# 1. CONFIGURACI√ìN\n",
        "# ==========================================================\n",
        "DRIVE_PROJECT_PATH = '/content/drive/MyDrive/TFM_Cienciometria'\n",
        "MODEL_NAME = model_selector.value\n",
        "\n",
        "# --- PAR√ÅMETROS DEL √çNDICE KI ---\n",
        "# [cite_start]Ventana de 5 a√±os hacia atr√°s y 5 hacia adelante, como en el paper [cite: 243]\n",
        "YEARS_WINDOW = 5\n",
        "\n",
        "# --- PAR√ÅMETRO DE OPTIMIZACI√ìN ---\n",
        "# 8192 o 16384 son buenos valores para una A100.\n",
        "BATCH_SIZE = 16384\n",
        "\n",
        "print(f\"‚úÖ Modelo seleccionado: {MODEL_NAME}\")\n",
        "print(f\"‚úÖ Ventana de c√°lculo para KI: {YEARS_WINDOW} a√±os (pasado y futuro)\")\n",
        "\n",
        "# Configurar rutas y directorios basados en la selecci√≥n\n",
        "if MODEL_NAME == 'mathbert':\n",
        "    TMP_DIR = os.path.join(DRIVE_PROJECT_PATH, 'data/mathbert/tmp')\n",
        "elif MODEL_NAME == 'word2vec':\n",
        "    TMP_DIR = os.path.join(DRIVE_PROJECT_PATH, 'data/word2vec/tmp')\n",
        "else:\n",
        "    # Este bloque se ejecuta si no se ha seleccionado un modelo v√°lido\n",
        "    pass\n",
        "\n",
        "# ==========================================================\n",
        "# 2. L√ìGICA DE C√ÅLCULO DEL √çNDICE KI SEM√ÅNTICO\n",
        "# ==========================================================\n",
        "def compute_ki_semantic(df: pd.DataFrame, years_window: int, batch_size: int):\n",
        "    \"\"\"\n",
        "    Calcula el √çndice KI Sem√°ntico usando una ventana temporal sim√©trica (pasado/futuro)\n",
        "    y vectores de embeddings pre-calculados. El c√°lculo se optimiza para GPU.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"üöÄ Realizando c√°lculos del √çndice KI en {device} con precisi√≥n FP16 y BATCH_SIZE={batch_size}\")\n",
        "\n",
        "    # --- Pre-c√°lculos de fechas y ORDENAMIENTO CR√çTICO ---\n",
        "    df['cover_date'] = pd.to_datetime(df['cover_date'], errors=\"coerce\")\n",
        "    df = df.dropna(subset=['cover_date', 'vector']).sort_values(by=[\"cover_date\", \"eid\"]).reset_index(drop=True)\n",
        "    print(\"‚úÖ DataFrame ordenado por 'cover_date' y 'eid'.\")\n",
        "\n",
        "    # Convertir fechas a n√∫meros para b√∫squedas r√°pidas con NumPy\n",
        "    dates_ordinal = df['cover_date'].apply(lambda x: x.toordinal()).to_numpy()\n",
        "\n",
        "    # Determinar el rango de fechas procesable para tener ventanas completas\n",
        "    time_delta = pd.Timedelta(days=365.25 * years_window)\n",
        "    min_date = df['cover_date'].min() + time_delta\n",
        "    max_date = df['cover_date'].max() - time_delta\n",
        "\n",
        "    # Filtrar para procesar solo los art√≠culos que tienen datos suficientes\n",
        "    processing_mask = (df['cover_date'] >= min_date) & (df['cover_date'] <= max_date)\n",
        "    processing_indices = df.index[processing_mask].to_numpy()\n",
        "\n",
        "    print(f\"‚ÑπÔ∏è  Corpus total: {len(df)} art√≠culos.\")\n",
        "    print(f\"‚úÖ Se procesar√°n {len(processing_indices)} art√≠culos entre {min_date.date()} y {max_date.date()} para garantizar ventanas completas.\")\n",
        "\n",
        "    # --- Carga y normalizaci√≥n de vectores en la GPU ---\n",
        "    V = np.vstack(df[\"vector\"].values).astype(np.float32)\n",
        "    V_gpu = torch.from_numpy(V).to(device, dtype=torch.float16)\n",
        "    V_gpu = V_gpu / torch.norm(V_gpu, dim=1, keepdim=True).clamp(min=1e-12)\n",
        "\n",
        "    results = []\n",
        "    desc = f\"üîç Calculando KI Sem√°ntico (Ventana={years_window} a√±os)\"\n",
        "\n",
        "    # Bucle principal por lotes (solo sobre los √≠ndices procesables)\n",
        "    for i in tqdm(range(0, len(processing_indices), batch_size), desc=desc):\n",
        "        batch_indices = processing_indices[i : i + batch_size]\n",
        "\n",
        "        for j in batch_indices:\n",
        "            target_vector = V_gpu[j]\n",
        "            current_date_ord = dates_ordinal[j]\n",
        "            window_days = time_delta.days\n",
        "\n",
        "            # --- Ventana PASADA ---\n",
        "            past_start_ord = current_date_ord - window_days\n",
        "            start_idx = np.searchsorted(dates_ordinal, past_start_ord, side='left')\n",
        "            past_indices = np.arange(start_idx, j)\n",
        "\n",
        "            if len(past_indices) > 0:\n",
        "                past_sims = (V_gpu[past_indices] @ target_vector + 1) / 2\n",
        "                sum_past_sim = torch.sum(past_sims, dtype=torch.float32).clamp(min=1e-9) # Clamp para evitar divisi√≥n por cero\n",
        "            else:\n",
        "                sum_past_sim = torch.tensor(1e-9, device=device)\n",
        "\n",
        "            # --- Ventana FUTURA ---\n",
        "            future_end_ord = current_date_ord + window_days\n",
        "            end_idx = np.searchsorted(dates_ordinal, future_end_ord, side='right')\n",
        "            future_indices = np.arange(j + 1, end_idx)\n",
        "\n",
        "            if len(future_indices) > 0:\n",
        "                future_sims = (V_gpu[future_indices] @ target_vector + 1) / 2\n",
        "                sum_future_sim = torch.sum(future_sims, dtype=torch.float32).clamp(min=1e-9)\n",
        "            else:\n",
        "                sum_future_sim = torch.tensor(1e-9, device=device)\n",
        "\n",
        "            # --- C√°lculo del √çndice KI Sem√°ntico ---\n",
        "            ki_score = torch.log(sum_future_sim / sum_past_sim)\n",
        "\n",
        "            results.append({\n",
        "                \"eid\": df.at[j, \"eid\"],\n",
        "                \"ki_score\": ki_score.item(),\n",
        "                \"past_pool_size\": len(past_indices),\n",
        "                \"future_pool_size\": len(future_indices)\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# ==========================================================\n",
        "# 3. EJECUCI√ìN PRINCIPAL\n",
        "# ==========================================================\n",
        "if __name__ == '__main__':\n",
        "    if MODEL_NAME != 'Seleccione un modelo':\n",
        "        t0 = time.time()\n",
        "        BASE = \"TODO\"\n",
        "\n",
        "        # Generar nombres de archivo\n",
        "        input_path = os.path.join(TMP_DIR, f\"04_vectores_{MODEL_NAME}_{BASE}.pkl\")\n",
        "        output_suffix = f\"05_ki_semantic_{MODEL_NAME}_{BASE}_win{YEARS_WINDOW}\"\n",
        "        csv_output = os.path.join(TMP_DIR, f\"{output_suffix}.csv\")\n",
        "        pkl_output = os.path.join(TMP_DIR, f\"{output_suffix}.pkl\")\n",
        "\n",
        "        print(\"=\"*50)\n",
        "        print(f\"INICIANDO C√ÅLCULO PARA EL MODELO: {MODEL_NAME}\")\n",
        "        print(f\"CARGANDO VECTORES DESDE: {input_path}\")\n",
        "        print(\"=\"*50)\n",
        "        df_main = pd.read_pickle(input_path)\n",
        "\n",
        "        # --- LLAMADA A LA FUNCI√ìN DE C√ÅLCULO ---\n",
        "        ki_df = compute_ki_semantic(\n",
        "            df=df_main,\n",
        "            years_window=YEARS_WINDOW,\n",
        "            batch_size=BATCH_SIZE\n",
        "        )\n",
        "\n",
        "        print(\"\\nüìä Resumen del √çndice KI Sem√°ntico:\")\n",
        "        if not ki_df.empty:\n",
        "            print(ki_df['ki_score'].describe())\n",
        "        else:\n",
        "            print(\"No se generaron resultados. Revisa el rango de fechas de tu corpus.\")\n",
        "\n",
        "        print(\"\\nüíæ Guardando resultados...\")\n",
        "        meta_cols = [c for c in [\"eid\", \"title\", \"cover_date\", \"citedby_count\", \"doi\"] if c in df_main.columns]\n",
        "        # Usamos 'inner' join para quedarnos solo con los art√≠culos para los que se pudo calcular el KI\n",
        "        final_df = df_main[meta_cols].merge(ki_df, on=\"eid\", how=\"inner\")\n",
        "\n",
        "        final_df.to_csv(csv_output, index=False)\n",
        "        final_df.to_pickle(pkl_output)\n",
        "\n",
        "        elapsed = time.time() - t0\n",
        "        print(f\"\\n‚úÖ Resultados guardados en:\\n- {csv_output}\\n- {pkl_output}\")\n",
        "        print(f\"‚è±Ô∏è  Tiempo total: {elapsed/60:.2f} min\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Por favor, selecciona un modelo en el men√∫ desplegable y vuelve a ejecutar la celda.\")"
      ]
    }
  ]
}